import collections
import os
import tarfile
import tempfile

import numpy as np
import pandas as pd
import pathlib
import statistics

from ete3 import Tree
from Bio import Phylo

from pypythia.prediction import get_all_features
from pypythia.msa import MSA
from pypythia.raxmlng import RAxMLNG
from pypythia.predictor import DifficultyPredictor

configfile: "config.yaml"

parquet_file_prefix = config["parquet_file_prefix"]
data_src = pathlib.Path(config["data_src"])
outdir = pathlib.Path(config["outdir"]) / parquet_file_prefix

msa_file_suffix = config["msa_file_suffix"]
empirical = config["empirical"]

raxml_ng = config["raxml_ng"]
radomness_tester = config["radomness_tester"]

predictor_path = config["predictor_path"]
predictor = DifficultyPredictor(open(predictor_path, "rb"))

data_src = pathlib.Path(data_src)
if not data_src.exists():
    data_src_compressed = data_src.with_suffix(f"{data_src.suffix}.tar.gz")
    if not data_src_compressed.exists():
        raise RuntimeError(f"Input data {data_src} does not exists and could not find a compressed directory either.")
    print("Found compressed input data, decompressing...")
    tar = tarfile.open(data_src_compressed, "r:gz")
    tar.extractall(path=data_src.parent)
    tar.close()

msa_files = pathlib.Path(data_src).rglob(f"*.{msa_file_suffix}")
msas = dict([(msa_file.name, msa_file.absolute()) for msa_file in msa_files])


def _get_stats_for_batch(brlen_batch, suffix):
    return {
        f"mean_{suffix}": statistics.mean(brlen_batch),
        f"median_{suffix}": statistics.median(brlen_batch),
        f"stdev_{suffix}": statistics.stdev(brlen_batch),
        f"total_{suffix}": sum(brlen_batch),
        f"min_{suffix}": min(brlen_batch),
        f"max_{suffix}": max(brlen_batch),
    }


def compute_brlen_statistics(newick_tree):
    tree = Tree(newick_tree)
    external_brlens = []
    internal_brlens = []

    internal_nodes = []

    for node in tree.traverse():
        if node.is_leaf():
            external_brlens.append(node.dist)
        else:
            internal_brlens.append(node.dist)
            internal_nodes.append(node)

    all_brlens = external_brlens + internal_brlens

    data = {
        **_get_stats_for_batch(external_brlens, "_external_brlens"),
        **_get_stats_for_batch(internal_brlens, "_internal_brlens"),
        **_get_stats_for_batch(all_brlens, "_all_brlens")
    }

    return data


def parse_randomness_log(logfile):
    randomness = {
        "entropy_rand": 0,
        "chi_2": 0,
        "mean_rand": 0,
        "mcpi": -1,
        "scc": 0,
        "comp": 0
    }
    for line in open(logfile).readlines():
        split_line = line.strip().split()
        if line.startswith("Entropy"):
            # Entropy = 0.000000 bits per byte
            randomness["entropy_rand"] = float(split_line[2]) / 8.0
        elif line.startswith("of this "):
            # Optimum compression would reduce the size
            # of this 0 byte file by 100 percent.
            randomness["comp"] = 1.0 - (float(split_line[6]) / 100.0)
        elif line.startswith("would exceed "):
            # Chi square distribution for 0 samples is nan, and randomly
            # would exceed this value nan percent of the times.
            if split_line[4] == "nan":
                continue
            if "less than" in line:
                randomness["chi_2"] = 1.0 - float(split_line[6])
            else:
                randomness["chi_2"] = 1.0 - float(split_line[4])
        elif line.startswith("Arithmetic mean "):
            # Arithmetic mean value of data bytes is nan (127.5 = random).
            if split_line[7] != "nan":
                randomness["mean_rand"] = float(split_line[7]) / 127.5
        elif line.startswith("Monte Carlo "):
            # Monte Carlo value for Pi is nan (error nan percent).
            if split_line[8] != "nan":
                randomness["mcpi"] = float(split_line[8]) / 100.0
        elif line.startswith("Serial correlation "):
            # Serial correlation coefficient is undefined (all values equal!).
            if split_line[4] != "undefined":
                randomness["scc"] = 1.0 - abs(float(split_line[4]))
    return randomness

rule all:
    input:
        os.path.join(outdir,parquet_file_prefix + ".parquet")


rule raxmlng_treesearch:
    output:
        raxml_bestTree = os.path.join(outdir, "{msa}", "raxmlng", "inference.raxml.bestTree")
    params:
        msa_file = lambda wildcards: msas[wildcards.msa],
        prefix = os.path.join(outdir, "{msa}", "raxmlng", "inference")
    log:
        raxml_log = os.path.join(outdir, "{msa}", "raxmlng", "inference.raxml.log")
    run:
        msa = MSA(params.msa_file)
        model = msa.get_raxmlng_model()

        shell(
            f"{raxml_ng} "
            "--search1 "
            "--msa {params.msa_file} "
            "--model {model} "
            "--prefix {params.prefix} "
            "--threads 2 "
            "> {log.raxml_log}"
        )


rule raxmlng_eval:
    input:
        raxml_bestTree = rules.raxmlng_treesearch.output.raxml_bestTree
    output:
        raxml_bestEvalTree = os.path.join(outdir, "{msa}", "raxmlng", "eval.raxml.bestTree")
    params:
        msa_file = lambda wildcards: msas[wildcards.msa],
        prefix = os.path.join(outdir, "{msa}", "raxmlng", "eval")
    log:
        raxml_log = os.path.join(outdir, "{msa}", "raxmlng", "eval.raxml.log")
    run:
        msa = MSA(params.msa_file)
        model = msa.get_raxmlng_model()

        shell(
            f"{raxml_ng} "
            "--eval "
            "--tree {input.raxml_bestTree} "
            "--msa {params.msa_file} "
            "--model {model} "
            "--prefix {params.prefix} "
            "--threads 2 "
            # "--redo "
            "> {log.raxml_log}"
        )


rule raxmlng_parsimony_tree:
    output:
        raxml_parsTree = os.path.join(outdir, "{msa}", "raxmlng", "parsimony.raxml.startTree")
    params:
        msa_file = lambda wildcards: msas[wildcards.msa],
        prefix = os.path.join(outdir,"{msa}", "raxmlng", "parsimony")
    log:
        raxml_log = os.path.join(outdir,"{msa}", "raxmlng", "parsimony.raxml.log")
    run:
        msa = MSA(params.msa_file)
        model = msa.get_raxmlng_model()

        shell(
            f"{raxml_ng} "
            "--start "
            "--tree pars{{1}} "
            "--msa {params.msa_file} "
            "--model {model} "
            "--prefix {params.prefix} "
            "--threads 2 "
            "> {log.raxml_log}"
        )


def traverse_tree_and_count_mutations(clade, msa_sequences, mutation_counter):
    """
    Traverses a tree recursively and counts the number of substitutions based on the parsimony rule.
    :param clade: current tree node
    :param msa: dict with map of leaf name to nucleotide sequence
    :param mutation_counter: list with per-site substitution counts
    """

    if clade.name:
        return [[char] for char in msa_sequences[clade.name]]

    pars_seq = []
    for c in clade.clades:
        if not pars_seq:
            pars_seq = traverse_tree_and_count_mutations(c, msa_sequences, mutation_counter)
        else:
            temp_pars_seq = traverse_tree_and_count_mutations(c, msa_sequences, mutation_counter)
            for i in range(len(pars_seq)):
                site1 = pars_seq[i]
                site2 = temp_pars_seq[i]

                intersection = list(set(site1) & set(site2))

                if len(intersection) == 0:
                    mutation_counter[i] += 1
                    intersection = site1 + site2

                pars_seq[i] = intersection
    return pars_seq


rule randomness:
    input:
        raxml_parsTree = rules.raxmlng_parsimony_tree.output.raxml_parsTree
    output:
        randomness = os.path.join(outdir, "{msa}", "randomness.txt")
    params:
        msa_file = lambda wildcards: msas[wildcards.msa]
    run:
        msa = MSA(params.msa_file)
        msa_dict = dict((seq.id, seq.seq) for seq in msa.msa)

        clade = Phylo.read(input.raxml_parsTree, "newick")
        mutation_counter = collections.defaultdict(int)
        traverse_tree_and_count_mutations(clade.root, msa_dict, mutation_counter)

        mutation_counter = [mutation_counter[i] for i in range(msa.number_of_sites())]

        # pad sequence if too short, reduce if too long (since the random sequence tester complains)
        if len(mutation_counter) < 10:
            mutation_counter += [0] * (10 - len(mutation_counter))

        mutation_counter = mutation_counter[:100_000]

        with tempfile.NamedTemporaryFile("wb+") as tmpfile:
            tmpfile.write(bytes(mutation_counter))

            # run the Fourmilab Random Sequence Tester
            shell(
                radomness_tester +
                " " +
                tmpfile.name +
                " > " + output.randomness
            )


rule collect_data:
    input:
        raxml_bestEvalTree = rules.raxmlng_eval.output.raxml_bestEvalTree,
        raxml_log = rules.raxmlng_treesearch.log.raxml_log,
        randomness_log = rules.randomness.output.randomness
    output:
        dataframe = os.path.join(outdir, "{msa}", "features.parquet")
    params:
        msa_file = lambda wildcards: msas[wildcards.msa]
    run:
        raxmlng = RAxMLNG(raxml_ng)
        msa = MSA(params.msa_file)
        features = get_all_features(raxmlng, msa, log_info=False)
        difficulty = predictor.predict(features)

        best_tree_newick = open(input.raxml_bestEvalTree).readline().strip()

        brlen_stats = compute_brlen_statistics(best_tree_newick)

        randomness_features = parse_randomness_log(input.randomness_log)

        all_data = {
            "msa_path": str(params.msa_file),
            "empirical": empirical,
            "data_source": parquet_file_prefix,
            "data_type": msa.data_type.value,
            "difficulty": difficulty,
            "num_patterns/num_sites": features["num_patterns"] / features["num_sites"],
            "pattern_entropy": features["bollback"] + features["num_sites"] * np.log2(features["num_sites"]),
            **features,
            **brlen_stats,
            **randomness_features
        }

        all_data = pd.DataFrame(data=all_data, index=[0])
        all_data.to_parquet(output.dataframe)


rule collect_all_dataframes:
    input:
        dataframes = expand(os.path.join(outdir,"{msa}","features.parquet"),msa=msas.keys()),
    output:
        dataframe = os.path.join(outdir,parquet_file_prefix + ".parquet")
    run:
        dfs = []
        for df_file in input.dataframes:
            dfs.append(pd.read_parquet(df_file))
        df = pd.concat(dfs, ignore_index=True)
        df.to_parquet(output.dataframe)

